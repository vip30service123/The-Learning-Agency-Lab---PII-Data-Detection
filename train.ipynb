{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForTokenClassification, IntervalStrategy, Trainer, TrainingArguments\n",
    "\n",
    "from src.const import label2id, id2label \n",
    "from src.dataset import DatasetForProcessedData\n",
    "from src.metric import MetricForPII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('params.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#### Prepare dataset.\")\n",
    "\n",
    "train_ds = DatasetForProcessedData(config.dataset.processed_train_path)\n",
    "test_ds = DatasetForProcessedData(config.dataset.processed_test_path)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(config.model.base_model_name_or_path,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tid2label=id2label,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel2id=label2id,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tfinetuning_task=\"ner\")\n",
    "\n",
    "print(\"#### Prepare config.\")\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=config.trainer.output_dir,          # output directory\n",
    "\tnum_train_epochs=config.trainer.num_train_epochs,              # total number of training epochs\n",
    "\tper_device_train_batch_size=config.trainer.per_device_train_batch_size,  # batch size per device during training\n",
    "\tper_device_eval_batch_size=config.trainer.per_device_eval_batch_size,   # batch size for evaluation\n",
    "\t# warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "\tweight_decay=config.trainer.weight_decay,               # strength of weight decay\n",
    "\tlogging_dir=config.trainer.logging_dir,            # directory for storing logs\n",
    "\n",
    "\tevaluation_strategy=config.trainer.evaluation_strategy,\n",
    "\tlogging_steps=config.trainer.logging_steps,\n",
    "\teval_steps=config.trainer.eval_steps,\n",
    "\t# evaluation_strategy = IntervalStrategy.STEPS\n",
    ")\n",
    "\n",
    "print(\"#### Prepare trainer.\")\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=train_ds,\n",
    "\teval_dataset=test_ds,\n",
    "\t# data_collator=data_collator,\n",
    "\t# tokenizer=tokenizer,\n",
    "\tcompute_metrics=MetricForPII(test_ds, 0.987, \"micro\")\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if config.model.save_model_path:\n",
    "\tmodel.save_pretrained(config.model.save_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
